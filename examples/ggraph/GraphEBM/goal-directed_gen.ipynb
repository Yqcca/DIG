{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of GraphEBM: Goal-Directed Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from dig.ggraph.dataset import ZINC250k, ZINC800\n",
    "from dig.ggraph.method import GraphEBM\n",
    "from dig.ggraph.evaluation import PropOptEvaluator, ConstPropOptEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dig.ggraph.evaluation import PropOptEvaluator, ConstPropOptEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_qed = ZINC250k(one_shot=True, root='./zinc250k_qed', prop_name='qed')\n",
    "splits = dataset_qed.get_split_idx()\n",
    "train_set_qed = dataset_qed[splits['train_idx']]\n",
    "train_dataloader_qed = DenseDataLoader(train_set_qed, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "# dataset_plogp = ZINC250k(one_shot=True, root='./zinc250k_plogp', prop_name='penalized_logp')\n",
    "# splits = dataset_plogp.get_split_idx()\n",
    "# train_set_plogp = dataset_plogp[splits['train_idx']]\n",
    "# train_dataloader_plogp = DenseDataLoader(train_set_plogp, batch_size=128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for _, batch in enumerate(train_dataloader_qed):\n",
    "    ### Dequantization\n",
    "    pos_x = batch.x.to(device).to(dtype=torch.float32)\n",
    "    pos_adj = batch.adj.to(device).to(dtype=torch.float32)\n",
    "    pos_x = pos_x.permute(0, 2, 1)\n",
    "    in_channels = 10\n",
    "    out_channels = 64\n",
    "    num_edge_type = 4\n",
    "    adj = pos_adj\n",
    "    h = pos_x # batchsize, ch, in\n",
    "\n",
    "    W = nn.Linear(in_channels, out_channels)\n",
    "    a = nn.Linear(2 * out_channels, 1)\n",
    "    asd = sum([adj[:, i, :, :] for i in range(4)])\n",
    "    ls = []\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting training, we need to define an object `graphebm` as an instance of class `GraphEBM`.\n",
    "\n",
    "**Skip training**: You can also download our trained models for goal-directed generation towards [QED](https://github.com/divelab/DIG_storage/blob/main/ggraph/GraphEBM/GraphEBM_zinc250k_goal_qed.pt) and [plogp](https://github.com/divelab/DIG_storage/blob/main/ggraph/GraphEBM/GraphEBM_zinc250k_goal_plogp.pt). Note: We found that we might have an error about loading the trained models if we download them with `wget`. If you have the same error, please download the models manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphebm = GraphEBM(n_atom=38, n_atom_type=10, n_edge_type=4, hidden=64, atype=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphebm.train_goal_directed(train_dataloader_qed, lr=1e-4, wd=0, max_epochs=20, c=0, ld_step=150, ld_noise=0.005, ld_step_size=30, clamp=True, alpha=1, save_interval=1, save_dir='./checkpoints_goal_qed', metric='drd2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphebm.train_goal_directed(train_dataloader_plogp, lr=1e-4, wd=0, max_epochs=20, c=0, ld_step=150, ld_noise=0.005, ld_step_size=30, clamp=True, alpha=1, save_interval=1, save_dir='./checkpoints_goal_qed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from dig.ggraph.dataset import ZINC250k, ZINC800\n",
    "from dig.ggraph.method import GraphEBM\n",
    "from dig.ggraph.evaluation import PropOptEvaluator, ConstPropOptEvaluator\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "dataset_qed = ZINC250k(one_shot=True, root='./zinc250k_qed', prop_name='qed')\n",
    "splits = dataset_qed.get_split_idx()\n",
    "train_set_qed = dataset_qed[splits['train_idx']]\n",
    "train_dataloader_qed = DenseDataLoader(train_set_qed, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "graphebm = GraphEBM(n_atom=38, n_atom_type=10, n_edge_type=4, hidden=64, atype=1, device=device)\n",
    "graphebm.train_goal_directed(train_dataloader_qed, lr=1e-4, wd=0, max_epochs=3, c=0, ld_step=150, ld_noise=0.005, ld_step_size=30, clamp=True, alpha=1, save_interval=1, save_dir='./checkpoints_goal_s1', metric='median1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4, 3, 3, 3, 2, 2, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_list = [1,2,3,3,6,4,3,2]\n",
    "sorted(prop_list)[::-1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct molecules from our generated node matrices and adjacency tensors, we need the `atomic_num_list`, which denotes what atom each dimension of the node matrix corresponds to. `0` denotes the virtual atom type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Ignore info output by RDKit\n",
    "# RDLogger.DisableLog('rdApp.error') \n",
    "# RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "# atomic_num_list = dataset_qed.atom_list\n",
    "# gen_mols = graphebm.run_rand_gen(checkpoint_path='./GraphEBM_zinc250k_goal_qed.pt', n_samples=10000, c=0, ld_step=150, ld_noise=0.005, ld_step_size=30, clamp=True, atomic_num_list=atomic_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "l.append(torch.tensor(0))\n",
    "l.append(torch.tensor(1))\n",
    "a = torch.tensor(l)\n",
    "b = torch.tensor(l)[:,None]\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running property optimization and the next constraint property optimization takes more time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDLogger.DisableLog('rdApp.error') \n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "atomic_num_list = dataset_qed.atom_list\n",
    "train_smiles = [data.smile for data in dataset_qed[splits['train_idx']]]\n",
    "initialization_loader_qed = DenseDataLoader(train_set_qed, batch_size=10000, shuffle=False, num_workers=0)\n",
    "\n",
    "save_mols_list, prop_list = graphebm.run_prop_opt('./GraphEBM_zinc250k_goal_qed.pt', initialization_loader=initialization_loader_qed, c=0, ld_step=300, ld_noise=0.005, ld_step_size=0.2, clamp=True, atomic_num_list=atomic_num_list, train_smiles=train_smiles, metric='drd2')\n",
    "print(prop_list)\n",
    "res_dict = {'mols':save_mols_list}\n",
    "evaluator = PropOptEvaluator(prop_name='drd2')\n",
    "results = evaluator.eval(res_dict)\n",
    "print(results)\n",
    "\n",
    "# from dig.ggraph.method.GraphEBM.energy_func import EnergyFunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraint Property Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zinc800 = ZINC800(one_shot=True, root='./zinc800_plogp')\n",
    "initialization_dataloader = DenseDataLoader(dataset_zinc800, batch_size=800, shuffle=True, num_workers=0)\n",
    "\n",
    "RDLogger.DisableLog('rdApp.error') \n",
    "RDLogger.DisableLog('rdApp.warning')\n",
    "\n",
    "train_smiles = [data.smile for data in dataset_zinc800]\n",
    "\n",
    "mols_0_list, mols_2_list, mols_4_list, mols_6_list, imp_0_list, imp_2_list, imp_4_list, imp_4_list = graphebm.run_const_prop_opt('./GraphEBM_zinc250k_goal_qed.pt', initialization_loader=initialization_dataloader, c=0, ld_step=500, ld_noise=0.005, ld_step_size=0.2, clamp=True, atomic_num_list=atomic_num_list, train_smiles=train_smiles)\n",
    "res_dict = {'inp_smiles': train_smiles, 'mols_0':mols_0_list, 'mols_2': mols_2_list, 'mols_4': mols_4_list, 'mols_6': mols_6_list}\n",
    "evaluator = ConstPropOptEvaluator()\n",
    "results = evaluator.eval(res_dict)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
